import pandas as pd
import xgboost as xgb
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROCESSED_DATA_PATH = os.path.join(BASE_DIR, 'data', 'processed')
MODEL_PATH = os.path.join(BASE_DIR, 'models')
os.makedirs(MODEL_PATH, exist_ok=True)

plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']
plt.rcParams['axes.unicode_minus'] = False

CONFIG = {
    'test_size': 0.2,
    'random_state': 42,
    'xgb_params': {
        'n_estimators': 200,        
        'max_depth': 6,
        'learning_rate': 0.05, 
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'objective': 'multi:softmax',
        'num_class': 3,
        'eval_metric': 'mlogloss',
        'tree_method': 'hist',        
        'random_state': 42
    }
}


def load_data(use_full_data=True): #modify: Switch on different dataset 
    filename = 'training_data_full.csv' if use_full_data else 'training_data_main.csv'
    data_path = os.path.join(PROCESSED_DATA_PATH, filename)
    print(f"ğŸ“‚ Loading Dataset: {filename}")
    return pd.read_csv(data_path)


def prepare_xy(df: pd.DataFrame):
    """
    Prepare feature matrix X and label vector y, with basic checks.

    æº–å‚™ç‰¹å¾µçŸ©é™£ X èˆ‡æ¨™ç±¤å‘é‡ yï¼Œä¸¦åšåŸºæœ¬æª¢æŸ¥ã€‚
    """
    if 'success_level' not in df.columns:
        raise ValueError("Label column 'success_level' is missing in data / è³‡æ–™ä¸­æ‰¾ä¸åˆ°æ¨™ç±¤æ¬„ä½ 'success_level'")

    # y: ensure it is integer type (0, 1, 2)
    # yï¼šç¢ºä¿ç‚ºæ•´æ•¸å‹åˆ¥ï¼ˆ0, 1, 2ï¼‰
    y = df['success_level'].astype(int)

    # X: drop label column and appid (appid is just an identifier)
    # Xï¼šç§»é™¤æ¨™ç±¤æ¬„ä½èˆ‡ appidï¼ˆappid åƒ…ä½œç‚ºè­˜åˆ¥ç”¨ï¼‰
    X = df.drop(columns=['success_level'])
    if 'appid' in X.columns:
        X = X.drop(columns=['appid'])

    # Keep only numeric columns (including one-hot encoded features)
    # åƒ…ä¿ç•™æ•¸å€¼å‹æ¬„ä½ï¼ˆåŒ…å« one-hot ç·¨ç¢¼å¾Œçš„æ¬„ä½ï¼‰
    X = X.select_dtypes(include=['number'])

    print("Feature count (ç‰¹å¾µæ¬„ä½æ•¸é‡):", X.shape[1])
    print("Example feature names (ç‰¹å¾µæ¬„ä½åç¨±ç¤ºä¾‹):", list(X.columns)[:20], "...")
    print("Label distribution (y åˆ†å¸ƒ):")
    print(
        y.value_counts().sort_index().rename(
            index={0: "Cold(0)", 1: "Normal(1)", 2: "Hot(2)"}
        )
    )

    # Sanity check for class labels
    # é¡åˆ¥æ¨™ç±¤å®‰å…¨æª¢æŸ¥ï¼Œç¢ºèªç‚ºé æœŸçš„ 0/1/2
    classes = sorted(y.unique())
    if classes != [0, 1, 2]:
        print("Warning: success_level classes are not [0, 1, 2], actual:", classes,
              "/ è­¦å‘Šï¼šç›®å‰ success_level é¡åˆ¥ä¸¦éé æœŸçš„ [0, 1, 2]ï¼Œå¯¦éš›ç‚ºä¸Šè¿°å…§å®¹")

    return X, y


def comprehensive_evaluation(model, X_train, X_test, y_train, y_test):
    """
    Perform comprehensive evaluation: confusion matrix, classification report, and cross-validation.

    é€²è¡Œå®Œæ•´æ¨¡å‹è©•ä¼°ï¼šåŒ…å«æ··æ·†çŸ©é™£ã€classification report èˆ‡äº¤å‰é©—è­‰ã€‚
    """
    y_pred = model.predict(X_test)

    # Confusion matrix
    # æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])
    plt.figure(figsize=(7, 5))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['Cold(0)', 'Normal(1)', 'Hot(2)'],
        yticklabels=['Cold(0)', 'Normal(1)', 'Hot(2)']
    )
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.tight_layout()
    plt.show()

    # Precision / Recall / F1 report
    # ç²¾ç¢ºç‡ / å¬å›ç‡ / F1 åˆ†æ•¸å ±å‘Š
    print("\nClassification Report (Precision / Recall / F1) / åˆ†é¡å ±å‘Šï¼š")
    print(classification_report(y_test, y_pred, digits=4))

    # 5-fold cross validation on training data (accuracy)
    # ä½¿ç”¨è¨“ç·´è³‡æ–™åš 5 æŠ˜äº¤å‰é©—è­‰ï¼ˆè©•ä¼°æŒ‡æ¨™ç‚º accuracyï¼‰
    print("\n5-Fold Cross Validation (accuracy) / 5 æŠ˜äº¤å‰é©—è­‰ï¼ˆæº–ç¢ºç‡ï¼‰ï¼š")
    xgb_for_cv = xgb.XGBClassifier(**CONFIG['xgb_params'])
    cv_scores = cross_val_score(xgb_for_cv, X_train, y_train, cv=5, scoring='accuracy')
    print(f"Scores: {cv_scores}")
    print(f"Avg: {cv_scores.mean():.4f}  (Â± {cv_scores.std() * 2:.4f})")


def train():
    """
    Main training entry point:
    1. Load data
    2. Prepare X / y
    3. Train-test split
    4. Train XGBoost model
    5. Evaluate and save model

    è¨“ç·´ä¸»æµç¨‹ï¼š
    1. è¼‰å…¥è³‡æ–™
    2. æº–å‚™ X / y
    3. åˆ‡åˆ†è¨“ç·´ / æ¸¬è©¦é›†
    4. è¨“ç·´ XGBoost æ¨¡å‹
    5. è©•ä¼°ä¸¦å„²å­˜æ¨¡å‹
    """
    # 1. Load data
    # 1. è¼‰å…¥è³‡æ–™
    df = load_data()

    # 2. Prepare X / y
    # 2. æº–å‚™ç‰¹å¾µèˆ‡æ¨™ç±¤
    X, y = prepare_xy(df)

    # 3. Train-test split with stratified sampling to keep class proportions
    # 3. ä»¥ stratify æ–¹å¼åˆ‡åˆ†è¨“ç·´ / æ¸¬è©¦é›†ï¼Œç¶­æŒé¡åˆ¥æ¯”ä¾‹
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=CONFIG['test_size'],
        random_state=CONFIG['random_state'],
        stratify=y
    )

    # 4. Build and train the XGBoost classifier
    # 4. å»ºç«‹ä¸¦è¨“ç·´ XGBoost åˆ†é¡æ¨¡å‹
    model = xgb.XGBClassifier(**CONFIG['xgb_params'])
    model.fit(X_train, y_train)

    # 5. Evaluation
    # 5. æ¨¡å‹è©•ä¼°
    comprehensive_evaluation(model, X_train, X_test, y_train, y_test)

    # 6. Save trained model to file
    # 6. å°‡è¨“ç·´å¥½çš„æ¨¡å‹å„²å­˜æˆæª”æ¡ˆ
    save_file = os.path.join(MODEL_PATH, 'xgb_model.json')
    model.save_model(save_file)
    print(f"\nModel saved to: {save_file} / æ¨¡å‹å·²å„²å­˜è‡³æ­¤è·¯å¾‘") #models/xgb_model.json


if __name__ == "__main__":
    train()